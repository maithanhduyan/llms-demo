// Generated by Copilot
// Cost (loss) functions for neural networks

pub enum CostFunction {
    CrossEntropy,
    Mse,
    Binary,
    Mae,
    Mape,
    Msle,
}

impl CostFunction {
    pub fn compute(&self, target: &[f64], output: &[f64]) -> f64 {
        match self {
            CostFunction::CrossEntropy => {
                let mut error = 0.0;
                for (t, o) in target.iter().zip(output.iter()) {
                    error -= t * (o.max(1e-15)).ln() + (1.0 - t) * (1.0 - o.max(1e-15)).ln();
                }
                error / output.len() as f64
            }
            CostFunction::Mse => {
                let mut error = 0.0;
                for (t, o) in target.iter().zip(output.iter()) {
                    error += (t - o).powi(2);
                }
                error / output.len() as f64
            }
            CostFunction::Binary => {
                let mut misses = 0.0;
                for (t, o) in target.iter().zip(output.iter()) {
                    if (t * 2.0).round() != (o * 2.0).round() {
                        misses += 1.0;
                    }
                }
                misses
            }
            CostFunction::Mae => {
                let mut error = 0.0;
                for (t, o) in target.iter().zip(output.iter()) {
                    error += (t - o).abs();
                }
                error / output.len() as f64
            }
            CostFunction::Mape => {
                let mut error = 0.0;
                for (t, o) in target.iter().zip(output.iter()) {
                    error += ((o - t) / t.max(1e-15)).abs();
                }
                error / output.len() as f64
            }
            CostFunction::Msle => {
                let mut error = 0.0;
                for (t, o) in target.iter().zip(output.iter()) {
                    error += t.max(1e-15).ln() - o.max(1e-15).ln();
                }
                error
            }
        }
    }
}
